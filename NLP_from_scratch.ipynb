{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning avancé : Deep Learning for NLP\n",
    "On va mettre au point nos premiers modèles Deep Learning appliqué au traitement du langage. Plutôt que de tout recoder de zéro, on va ici s'appuyer sur la puissance de la librairie Keras qui nous permettra de connecter les layers à la volée et d'implémenter des architectures plus exotiques.\n",
    "\n",
    "Après cela vous saurez:\n",
    "- utiliser un embedding pré-calculé\n",
    "- construire un réseau de neurones avec Keras\n",
    "- construire une architecture custom avec Keras\n",
    "\n",
    "Cet exercice constitue également le test noté du cours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.pos as pos \n",
    "import numpy as np\n",
    "import pandas\n",
    "data='data/'\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du problème\n",
    "Le POS-Tagging et le Shallow Parsing constituent deux tâches classiques en NLP :\n",
    "- POS-Tagging : affecte à chaque mot un tag unique qui indique son rôle syntaxique (nom, verbe, adverbe, ..)\n",
    "- Shallow Parsing : affecte à chaque segment de phrase un tag unique qui indique le rôle de l'élément syntaxique auquel il appartient (groupe nominal, groupe verbal, etc..)\n",
    "Les fichiers sont ici au format ConLL : un mot par ligne, les phrases sont séparées par un saut de ligne.\n",
    "\n",
    "Nous allons ici refaire les travaux de l'article NLP almost from scratch, qui consiste à créer un réseau de neurones pour effectuer chaque tâche, puis nous ferons un modèle partagé et enfin un modèle hiearchique.\n",
    "\n",
    "<img src=\"MLP.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Modèle simple pour les tâches POS-Tagging et Shallow Parsing</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging\n",
    "## Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide des fonctions d'aide, importer les données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(r'C:\\Users\\Yasmina\\Google Drive\\Etudes\\Paris 1\\Master 2\\S2\\ML avancé\\TP\\machine-learning-avance\\assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordvector shape is (100003, 50)\n"
     ]
    }
   ],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère également les tags et on crée les dictionnaires adéquats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = {v:k for k,v in num_to_tag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ADJ', 1: 'ADJWH', 2: 'ADV', 3: 'ADVWH', 4: 'CC', 5: 'CLO', 6: 'CLR', 7: 'CLS', 8: 'CS', 9: 'DET', 10: 'DETWH', 11: 'ET', 12: 'I', 13: 'NC', 14: 'NPP', 15: 'P', 16: 'P+D', 17: 'P+PRO', 18: 'PONCT', 19: 'PREF', 20: 'PRO', 21: 'PROREL', 22: 'PROWH', 23: 'VINF', 24: 'VPR', 25: 'VPP', 26: 'V', 27: 'VS', 28: 'VIMP'}\n",
      "{'ADJ': 0, 'ADJWH': 1, 'ADV': 2, 'ADVWH': 3, 'CC': 4, 'CLO': 5, 'CLR': 6, 'CLS': 7, 'CS': 8, 'DET': 9, 'DETWH': 10, 'ET': 11, 'I': 12, 'NC': 13, 'NPP': 14, 'P': 15, 'P+D': 16, 'P+PRO': 17, 'PONCT': 18, 'PREF': 19, 'PRO': 20, 'PROREL': 21, 'PROWH': 22, 'VINF': 23, 'VPR': 24, 'VPP': 25, 'V': 26, 'VS': 27, 'VIMP': 28}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'ADJ'), (1, 'ADJWH'), (2, 'ADV'), (3, 'ADVWH'), (4, 'CC'), (5, 'CLO'), (6, 'CLR'), (7, 'CLS'), (8, 'CS'), (9, 'DET'), (10, 'DETWH'), (11, 'ET'), (12, 'I'), (13, 'NC'), (14, 'NPP'), (15, 'P'), (16, 'P+D'), (17, 'P+PRO'), (18, 'PONCT'), (19, 'PREF'), (20, 'PRO'), (21, 'PROREL'), (22, 'PROWH'), (23, 'VINF'), (24, 'VPR'), (25, 'VPP'), (26, 'V'), (27, 'VS'), (28, 'VIMP')])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(num_to_tag)\n",
    "print(tag_to_num)\n",
    "num_to_tag.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des documents puis création des matrices X_train, y_train, X_test, y_test\n",
    "le paramètre wsize précise la taille de la fenêtre, choisissez une valeur par défaut parmis (3,5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = du.load_dataset(data+'train.txt') # liste qui contient les phrases et les tags\n",
    "X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=7) # parcourt la liste et créer les matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  0, 13, ..., 15, 13, 18])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[0]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = du.load_dataset(data+'test.txt')\n",
    "X_test, y_test = du.docs_to_windows(\n",
    "    docs_test, word_to_num, tag_to_num, wsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 29)\n",
    "y_test = keras.utils.to_categorical(y_test, 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On extrait les lignes dans y_test qui contiennent des mots non-présents dans X_train : out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_oov,Y_test_oov = du.get_oov(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train a pour dimension (680238, 7)\n",
      "X_test a pour dimension (267334, 7)\n",
      "y_train a pour dimension (680238, 29)\n",
      "y_test a pour dimension (267334, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train a pour dimension\",X_train.shape)\n",
    "print(\"X_test a pour dimension\",X_test.shape)\n",
    "print(\"y_train a pour dimension\",y_train.shape)\n",
    "print(\"y_test a pour dimension\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37073, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_oov.shape)\n",
    "#Y_test_oov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Expliquer ce que contient X et Y ci-dessus ainsi que leur dimension. (NB: Lorsque l'on définit une fenêtre on est amené à créer un tag pour le début et la fin de phrase afin que les premiers et derniers mots puissent être considérés dans le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X représente les tags avec les mots qui correspondent à ces tags, le nombre de lignes correspond au nombre de mots uniques\n",
    "#dans vocab, le nombre de colonne correspond à la taille de la fenêtre choisie (window size)\n",
    "\n",
    "#chop your corpus into windows of w words (ex:3 words) where the center word is the one you want to classify\n",
    "#the other ones are context. Let's call this part of the data X.\n",
    "\n",
    "#fro each window, get the POS tag for the center word let's call this part of data y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne :\n",
    "Compléter le code ci-dessous pour définir une architecture :\n",
    "Embed (dim 50) -> Dense -> Dropout -> Predict (Softmax).\n",
    "N'hésitez pas à consulter l'aide de Keras pour :\n",
    "    - models.Sequential\n",
    "    - layers.Embedding\n",
    "    - layers.Flatten\n",
    "\n",
    "NB : Prendre garde à bien laisser l'embedding \"entrainable\" afin que la représentation vectorielle bénéficie aussi de la backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here\n",
    "e = Embedding(X_train.shape[0], X_train.shape[1], weights=[X_train], input_length=X_train.shape[1], trainable=True)\n",
    "model.add(e)\n",
    "#input layer \n",
    "model.add(Dense(768, input_shape=(1500,), activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "# hidden layer\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "# output layer\n",
    "model.add(Dense(29, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###fenetre = 3\n",
    "\n",
    "21258/21258 [==============================] - 308s 14ms/step - loss: 1.1530 - accuracy: 0.6505\n",
    "Epoch 2/3\n",
    "21258/21258 [==============================] - 306s 14ms/step - loss: 0.8031 - accuracy: 0.7579\n",
    "Epoch 3/3\n",
    "21258/21258 [==============================] - 290s 14ms/step - loss: 0.8264 - accuracy: 0.7550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here\n",
    "e = Embedding(680238, 3, weights=[X_train], input_length=3, trainable=True)\n",
    "model.add(e)\n",
    "#input layer\n",
    "model.add(Dense(100, input_dim=3, activation=\"relu\"))\n",
    "model.add(Dense(100, input_dim=3, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "# hidden layer\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "# output layer\n",
    "model.add(Dense(29, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### fenetre = 7 avec relu\n",
    "Epoch 1/3\n",
    "21258/21258 [==============================] - 597s 28ms/step - loss: 1.1110 - accuracy: 0.6712\n",
    "Epoch 2/3\n",
    "21258/21258 [==============================] - 598s 28ms/step - loss: 0.7648 - accuracy: 0.7789\n",
    "Epoch 3/3\n",
    "21258/21258 [==============================] - 551s 26ms/step - loss: 0.7961 - accuracy: 0.7733\n",
    "        \n",
    "     ### fenetre = 7 avec tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here\n",
    "e = Embedding(680238, 7, weights=[X_train], input_length=7, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Dense(100, input_dim=7, activation=\"tanh\"))\n",
    "model.add(Dense(100, input_dim=7, activation=\"tanh\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### fenetre = 5 avec tanh\n",
    "Epoch 1/3\n",
    "21258/21258 [==============================] - 468s 22ms/step - loss: 0.9490 - accuracy: 0.7221\n",
    "Epoch 2/3\n",
    "21258/21258 [==============================] - 431s 20ms/step - loss: 0.6892 - accuracy: 0.7926\n",
    "Epoch 3/3\n",
    "21258/21258 [==============================] - 424s 20ms/step - loss: 0.6542 - accuracy: 0.8032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# Your code Here\n",
    "e = Embedding(680238, 5, weights=[X_train], input_length=5, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Dense(100, input_dim=5, activation=\"tanh\"))\n",
    "model.add(Dense(100, input_dim=5, activation=\"tanh\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(80, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(60, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dense(40, activation='tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :\n",
    "- A quoi servent les layers suivants :\n",
    "    - Flatten()\n",
    "    - Dropout()\n",
    "- Combien de paramètres seront appris au cours de l'entrainement? (il existe une commande qui permette de trouver l'architecture du réseau de neurones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La layer Flatten() sert à remodèler le tenseur pour avoir la forme qui est égale au nombre d'éléments contenus dans le tenseur sans inclure la dimension du batch\n",
    "\n",
    "- La layer Dropout() définit de manière aléatoire les unités d'entrée à 0 avec une fréquence de fréquence à chaque étape pendant le temps d'entraînement, ce qui permet d'éviter le surajustement. Les entrées non définies sur 0 sont augmentées de 1 / (taux 1) de sorte que la somme de toutes les entrées reste inchangée.\n",
    "\n",
    "- le nombre de paramètres appris au cours de l'entraînement est de 3,464,409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 7, 7)              4761666   \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 7, 100)            800       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 7, 100)            10100     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 100)            0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 7, 80)             8080      \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 7, 80)             6480      \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 7, 80)             6480      \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 7, 60)             4860      \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 7, 60)             3660      \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 7, 60)             3660      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 7, 50)             3050      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 7, 50)             2550      \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 7, 50)             2550      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 7, 40)             2040      \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 7, 40)             1640      \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 7, 40)             1640      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 7, 40)             0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 280)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 29)                8149      \n",
      "=================================================================\n",
      "Total params: 4,827,405\n",
      "Trainable params: 4,827,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne : Compilation\n",
    "Compléter le code ci-dessous en choisissant l'optimizer RMSprop, et en choissisant la bonne loss (NB on est sur un probléme de classification à 29 modalités). La métrique sera renvoyée par les logs au cours de l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'RMSprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne  : Entrainement\n",
    "Compléter le code ci-dessous pour entrainer sur le couple X_train, Y_train et en validant sur X_test, Y_test. Vous devriez pouvoir atteindre une accuracy de 93% au bout de 3 itérations. N'oubliez pas que la taille du batch correspond au nombre d'exemples utilisés pour estimer le gradient. Il peut influer sur la convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "21258/21258 [==============================] - 622s 29ms/step - loss: 0.9157 - accuracy: 0.7312\n",
      "Epoch 2/3\n",
      "21258/21258 [==============================] - 605s 28ms/step - loss: 0.6485 - accuracy: 0.8053\n",
      "Epoch 3/3\n",
      "21258/21258 [==============================] - 609s 29ms/step - loss: 0.6248 - accuracy: 0.8124\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "- Nous avons décidé de regarder l'accuracy comme métrique. Expliquez en quoi ce choix est discutable.\n",
    "- On se propose de tester la performance sur des données qui contiennent des mots non-vus lors du train dans la cellule ci-dessous lancer l'évaluation sur X_test_oov,Y_test_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- on pourrait regarder le f1 score qui est une moyenne arithmétique de la précision et du recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 3s 3ms/step - loss: 0.6409 - accuracy: 0.8107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6409143209457397, 0.810655415058136]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_oov,Y_test_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Déterminer quelle est la meilleur taille de fenêtre selon vous (tester les plusieurs hypothèses parmis 3, 5 et 7)\n",
    "- Reprendre le code pour les différentes hypothèses suivantes avec la taille de fenêtre choisie ci-dessus :\n",
    "    - 1 embedding random non entrainable\n",
    "    - 2 embedding pré-entrainer (wv) entrainable\n",
    "    - 3 embedding pré-entrainer (wv) non entrainable\n",
    "- Conclure en testant sur X_test_oov et Y_test_oov :\n",
    "    - Quelle est la méthode que vous choisiriez et pourquoi? \n",
    "    - Pouvez-vous expliquer intuitivement les différents résultats ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Après plusieurs tests nous concluons que la meilleure taille de fenêtre est une taille de : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothèse 1 : embedding random non entrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothèse 2 : embedding pré-entrainer (wv) entrainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothèse 3 : embedding pré-entrainer (wv) non entrainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de sauvegarder le modèle (les poids W) dans un fichier .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_POStagging_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Reprendre la méthodologie ci-dessus et entrainer un modèle de shallow parsing\n",
    "- Les tags sont ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "- Les fichiers sont train_chunk.txt et test_chunk.txt\n",
    "- N'oubliez pas de recréer les tests out-of-vocabulary (X_test_oov et Y_test_oov)\n",
    "- Conclure sur votre choix du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordvector shape is (100003, 50)\n"
     ]
    }
   ],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames =  ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ','I-CONJ',\n",
    "             'B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = {v:k for k,v in num_to_tag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = du.load_dataset(data+'train_chunk.txt') # liste qui contient les phrases et les tags\n",
    "X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=5) # parcourt la liste et créer les matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = du.load_dataset(data+'test_chunk.txt')\n",
    "X_test, y_test = du.docs_to_windows(\n",
    "    docs_test, word_to_num, tag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 29)\n",
    "y_test = keras.utils.to_categorical(y_test, 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_oov,Y_test_oov = du.get_oov(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train a pour dimension (680238, 5)\n",
      "X_test a pour dimension (267334, 5)\n",
      "y_train a pour dimension (680238, 29)\n",
      "y_test a pour dimension (267334, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train a pour dimension\",X_train.shape)\n",
    "print(\"X_test a pour dimension\",X_test.shape)\n",
    "print(\"y_train a pour dimension\",y_train.shape)\n",
    "print(\"y_test a pour dimension\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_oov.shape)\n",
    "#Y_test_oov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here#\n",
    "e = Embedding(680238, 5, weights=[X_train], input_length=5, trainable=True)\n",
    "model.add(e)\n",
    "#model.add(Dense(10, input_dim=5, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'RMSprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21258/21258 [==============================] - 443s 21ms/step - loss: 70.5901 - accuracy: 0.2314\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 1s 1ms/step - loss: 13.0831 - accuracy: 0.2510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.083104133605957, 0.2510192096233368]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_oov,Y_test_oov, batch_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full multi tagged (POS tagging + Shallow parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning, il est possible et parfois même recommandé d'entrainer un réseau de neurones sur plusieurs tâches en même temps. Intuitivement on se dit que l'apprentissage de représentation sur une tâche devrait pouvoir aider sur une autre tâche. Cela permet en outre de disposer de plus de données par exemple et/ou d'avoir un modèle plus robuste, plus précis.\n",
    "\n",
    "Dans cette partie, on va s'appuyer sur la classe Model de Keras https://keras.io/getting-started/functional-api-guide/. Plutôt que d'ajouter les layers, il s'agit plutôt de voire chaque layer comme une fonction. Il s'agit alors d'enchainer les fonctions.\n",
    "\n",
    "<img src=\"mtl_images.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Un exemple d'architecture pour apprendre 3 tâches </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(\n",
    "      data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_postag = dict(enumerate(postagnames))\n",
    "postag_to_num = {v:k for k,v in num_to_postag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunktagnames = ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "num_to_chunktag = dict(enumerate(chunktagnames))\n",
    "chunktag_to_num = {v:k for k,v in num_to_chunktag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset(data+'train.txt')\n",
    "X_train, pos_train = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'train_chunk')\n",
    "X_train, chunk_train = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = du.load_dataset(data+'test.txt')\n",
    "X_test, pos_test = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'test_chunk')\n",
    "X_test, chunk_test = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en forme pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train=X_train[:400000,:]\n",
    "\n",
    "X_chunk_train=X_train[-400000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos_train = keras.utils.to_categorical(pos_train[0:400000,], 29)\n",
    "Y_pos_test = keras.utils.to_categorical(pos_test, 29)\n",
    "Y_chunk_train = keras.utils.to_categorical(chunk_train, 15)\n",
    "Y_chunk_test = keras.utils.to_categorical(chunk_test[-400000:,], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "Jusqu'à présent on dispose de :\n",
    "- wv qui contient un embedding\n",
    "- X_pos_ et Y_pos_ qui contiennent respectivement les mots et le tag sur train et test pour le pos tagging\n",
    "- X_chunk_ et Y_chunk_ qui contiennent respectivement les mots et le tag sur train et test pour le shallow parsing\n",
    "- X_pos et X_chunk n'ont aucune fenêtre en commun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du réseaux de neurones commun aux deux tâches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Utiliser la même architecture que précedemment pour construire le réseau de neurones communs : shared_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_nn=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Définir les inputs du réseaux de neurones communs à l'aide de la fonction Input. Ce sont les input layer de notre architecture, chacun correspond à une tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_input = keras.Input(shape=(784,))\n",
    "chunk_input = keras.Input(shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- Définir pos_representation et chunk_representation comme les images respectibes de chaque input par shared_nn\n",
    "- Définir pos_target et chunk_target comme l'image des représentations ci-dessus par des layers dense avec activation softmax (cf  la fonction Dense)\n",
    "\n",
    "Rappel : la fonction softmax va calculer $n$ softmax, où $n$ est le nombre de classe en suivant la formule :\n",
    "$$\\sigma(z)_j = \\frac{\\exp(z_j)}{\\sum_{i}\\exp(z_i)},$$\n",
    "avec $z$ le vecteur en sortie du réseau de neurones et $z_i$ sa $i$-ième composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_representation =\n",
    "chunk_representation =\n",
    "\n",
    "pos_target = \n",
    "chunk_target = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement et conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne \n",
    "Jusqu'ici on a juste défini la succession d'opération, il s'agit maintenant de créer le modèle en utilisant la classe modèle de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Model(inputs=,outputs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- compiler le modèle (NB : il y a une loss pour chaque tâche)\n",
    "- afficher l'architecture\n",
    "- entrainer le modèle pour le nombre d'époque suffisant en validant sur X_test et Y_test et une taille de batch de 128\n",
    "- Conclure en comparant avec les performances précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical learning\n",
    "Une autre façon de faire du multi-task consiste à construire une architecture en cascade où les tâches n'interviennent pas à la même profondeur du réseau de neurones.\n",
    "\n",
    "### Consigne :\n",
    "- En vous inspirant de la partie précedente, entrainer un modèle en cascade de type : \n",
    "                              POS\n",
    "                            /\n",
    "EMBEDDING - DENSE - DROPOUT \n",
    "                            \\\n",
    "                              DENSE - DROPOUT - CHUNK\n",
    "                              \n",
    "Autrement dit, dans cette approche, on pré-suppose que  \n",
    "- EMBEDDING - DENSE - DROPOUT apprend une représentation suffisante pour prédire le POS-TAG\n",
    "- plus de layer sont nécessaires pour le Shallow parsing\n",
    "- la représentation intermédiaire EMBEDDING - DENSE - DROPOUT est un bon point de départ pour le shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question optionnelle \n",
    "- Comment feriez vous, en utilisant la classe Model, pour pouvoir extraire l'embedding tuné sur le modèle pour un mot donné? Tenter de l'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
